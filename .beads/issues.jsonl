{"id":"bytefreq-1sd","title":"Update CLAUDE.md with Excel architecture","description":"Update CLAUDE.md to document:\n- Excel processing architecture using calamine\n- How Excel data flows through the system\n- Sheet selection mechanism\n- Data type conversion strategy (Excel types -\u003e strings)\n- Optional feature flag setup\n- Key functions in src/excel.rs module\n- Integration points with existing tabular pipeline\n\nAdd to 'Data Processing Pipeline' section and 'Common Development Patterns' section.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:12.613308Z","updated_at":"2025-12-12T20:41:14.739328Z","closed_at":"2025-12-12T20:41:14.739328Z","dependencies":[{"issue_id":"bytefreq-1sd","depends_on_id":"bytefreq-l05","type":"blocks","created_at":"2025-12-12T19:20:24.516465Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-37h","title":"Update README with native Excel support","description":"Update README.md to:\n- Add section on native Excel support (after 'Processing Microsoft Excel Files' section)\n- Document how to build with Excel feature: cargo build --release --features excel\n- Provide usage examples for .xlsx, .xls, .xlsb, .ods files\n- Show sheet selection examples (--sheet, --sheet-name)\n- Update feature list to mention native Excel support\n- Note that Excel feature is optional (requires feature flag)\n\nKeep existing Excel conversion section for users without the feature enabled.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:12.427465Z","updated_at":"2025-12-12T20:41:14.738499Z","closed_at":"2025-12-12T20:41:14.738499Z","dependencies":[{"issue_id":"bytefreq-37h","depends_on_id":"bytefreq-l05","type":"blocks","created_at":"2025-12-12T19:20:24.489399Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-4tb","title":"Add native Excel file support using calamine","description":"Implement native Excel (.xlsx, .xls, .xlsb, .ods) file reading support using the calamine crate. This eliminates the need for external conversion tools.\n\nDependencies already added:\n- calamine = { version = \"0.26\", optional = true }\n- Feature flag 'excel' created\n\nRemaining work tracked in subtasks.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-12T19:19:43.882543Z","updated_at":"2025-12-12T21:42:52.551841Z","closed_at":"2025-12-12T21:42:52.551841Z","dependencies":[{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-55j","type":"blocks","created_at":"2025-12-12T19:20:24.822744Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-dq0","type":"blocks","created_at":"2025-12-12T19:20:24.851021Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-l05","type":"blocks","created_at":"2025-12-12T19:20:24.877624Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-37h","type":"blocks","created_at":"2025-12-12T19:20:24.904133Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-1sd","type":"blocks","created_at":"2025-12-12T19:20:24.931923Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-4tb","depends_on_id":"bytefreq-p9s","type":"blocks","created_at":"2025-12-12T19:20:24.957501Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-55j","title":"Create Excel reader module (src/excel.rs)","description":"Create src/excel.rs module that:\n- Opens Excel workbooks using calamine\n- Iterates over sheets and rows\n- Converts Excel data to tabular format compatible with existing pipeline\n- Handles multiple data types (strings, numbers, dates, bools, errors)\n- Returns iterator or Vec\u003cVec\u003cString\u003e\u003e for processing\n\nKey considerations:\n- First row should be header (consistent with tabular format)\n- Handle empty cells appropriately\n- Convert all cell types to strings for masking\n- Support lazy loading for large files where possible","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:11.852258Z","updated_at":"2025-12-12T19:38:56.006015Z","closed_at":"2025-12-12T19:38:56.006015Z"}
{"id":"bytefreq-cpf","title":"Add Parquet file format support","description":"## Feature: Add Parquet file format support\n\n### Overview\nAdd support for reading Apache Parquet files as a new input format. Parquet data is converted to JSON internally to leverage the existing nested data processing pipeline with dot-notation paths.\n\n### Architecture Decision\nUnlike Excel (which converts to tabular format), Parquet routes through the **JSON processing path** because:\n- Parquet supports nested structs, lists, and maps\n- These map naturally to JSON objects and arrays\n- bytefreq's JSON processor already handles nested structures with paths like `user.address.city`\n\n### Library Choice\nUse official `parquet` crate from apache/arrow-rs (NOT deprecated parquet2):\n- Actively maintained by Apache\n- WASM-compatible (proven by parquet-wasm project)\n- v53+ recommended\n\n### Data Flow\n```\nParquet File\n    ↓\nParquetReader::read_as_json_lines()\n    ↓\nVec\u003cString\u003e of JSON lines\n    ↓\nprocess_json_line() [existing code]\n    ↓\nFrequency maps with dot-notation paths\n    ↓\nDQ Report output\n```\n\n### Child Tasks\n1. **cpf.1**: Add parquet crate as optional dependency\n2. **cpf.2**: Create src/parquet.rs reader module\n3. **cpf.3**: Integrate Parquet format in main.rs CLI\n4. **cpf.4**: Handle Parquet nested types to JSON conversion\n5. **cpf.5**: Test Parquet support with sample files\n\n### Usage (when complete)\n```bash\n# Build with parquet support\ncargo build --release --features parquet\n\n# Profile a parquet file\n./bytefreq -f parquet --parquet-path data.parquet\n\n# With low-grain unicode masking\n./bytefreq -f parquet --parquet-path data.parquet -g LU\n\n# Enhanced JSON output\n./bytefreq -f parquet --parquet-path data.parquet -e\n```\n\n### Success Criteria\n- [ ] `cargo build --features parquet` compiles\n- [ ] `cargo build --features parquet --target wasm32-unknown-unknown` compiles\n- [ ] Flat parquet files produce correct DQ output\n- [ ] Nested struct columns produce dot-notation paths\n- [ ] List columns produce array-indexed paths\n- [ ] No regressions in existing tabular/json/excel functionality","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-22T11:11:37.17573Z","updated_at":"2026-01-24T13:27:12.826446Z"}
{"id":"bytefreq-cpf.1","title":"Add parquet crate as optional dependency","description":"## Task: Add parquet crate as optional dependency\n\n### Files to Modify\n- `Cargo.toml` (lines 23, 27-30)\n\n### Exact Changes Required\n\n1. **Add dependency** (after line 23, following calamine pattern):\n```toml\nparquet = { version = \"53\", default-features = false, features = [\"arrow\", \"snap\", \"zstd\", \"lz4\"], optional = true }\narrow = { version = \"53\", default-features = false, optional = true }\n```\n\n2. **Add feature flag** (in [features] section, after line 29):\n```toml\nparquet = [\"dep:parquet\", \"dep:arrow\"]\n```\n\n### Reference Pattern\nCopy the pattern from lines 23 and 29:\n```toml\n# Line 23 pattern:\ncalamine = { version = \"0.26\", optional = true }\n# Line 29 pattern:\nexcel = [\"calamine\"]\n```\n\n### Acceptance Criteria\n- [ ] `cargo build --release` succeeds (without parquet feature)\n- [ ] `cargo build --release --features parquet` succeeds\n- [ ] `cargo build --release --features parquet --target wasm32-unknown-unknown` succeeds (WASM check)\n- [ ] Feature flag named exactly \"parquet\" (not \"parquet-support\" or similar)\n\n### Verification Commands\n```bash\ncargo build --release\ncargo build --release --features parquet\nrustup target add wasm32-unknown-unknown  # if not installed\ncargo build --release --features parquet --target wasm32-unknown-unknown\n```\n\n### DO NOT\n- Add unnecessary features to parquet crate\n- Change existing dependencies\n- Modify any other files","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-22T11:11:55.027142Z","updated_at":"2026-01-24T12:34:44.241366Z","dependencies":[{"issue_id":"bytefreq-cpf.1","depends_on_id":"bytefreq-cpf","type":"parent-child","created_at":"2026-01-22T11:11:55.027513Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-cpf.2","title":"Create src/parquet.rs reader module","description":"## Task: Create src/parquet.rs reader module\n\n### Files to Create/Modify\n- **Create**: `src/parquet.rs` (new file)\n- **Modify**: `src/lib.rs` (add module export, line 6-7 pattern)\n\n### Reference Pattern\nFollow `src/excel.rs` structure EXACTLY. The file has 3 sections:\n1. Lines 1-115: Feature-enabled implementation with `#[cfg(feature = \"parquet\")]`\n2. Lines 117-148: Dummy implementation with `#[cfg(not(feature = \"parquet\"))]`\n3. Lines 150-167: Tests with `#[cfg(test)] #[cfg(feature = \"parquet\")]`\n\n### Required Implementation\n\n#### Section 1: Feature-enabled implementation\n```rust\n#[cfg(feature = \"parquet\")]\nuse parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;\n#[cfg(feature = \"parquet\")]\nuse arrow::array::Array;\n#[cfg(feature = \"parquet\")]\nuse std::fs::File;\nuse std::path::Path;\n\n#[cfg(feature = \"parquet\")]\npub struct ParquetReader;\n\n#[cfg(feature = \"parquet\")]\nimpl ParquetReader {\n    /// Read Parquet file and return as JSON lines (one JSON object per row)\n    /// This feeds into the existing JSON processing pipeline\n    pub fn read_as_json_lines\u003cP: AsRef\u003cPath\u003e\u003e(\n        path: P,\n    ) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        // Implementation here - see cpf.4 for type conversion details\n    }\n}\n```\n\n#### Section 2: Dummy implementation (when feature disabled)\n```rust\n#[cfg(not(feature = \"parquet\"))]\npub struct ParquetReader;\n\n#[cfg(not(feature = \"parquet\"))]\nimpl ParquetReader {\n    pub fn read_as_json_lines\u003cP: AsRef\u003cPath\u003e\u003e(\n        _path: P,\n    ) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e {\n        Err(\"Parquet support not enabled. Rebuild with --features parquet\".into())\n    }\n}\n```\n\n#### Update lib.rs (after line 7)\nAdd module export following the excel pattern at lines 6-7:\n```rust\n#[cfg(feature = \"parquet\")]\npub mod parquet;\n```\n\n### Key Differences from Excel\n| Excel | Parquet |\n|-------|---------|\n| Returns `Vec\u003cVec\u003cString\u003e\u003e` (rows of columns) | Returns `Vec\u003cString\u003e` (JSON lines) |\n| Converts to tabular delimiter format | Converts to JSON objects |\n| Uses `process_csv_line()` downstream | Uses `process_json_line()` downstream |\n\n### API Design\n```rust\n// Main entry point - returns JSON lines for the JSON processing pipeline\npub fn read_as_json_lines\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cVec\u003cString\u003e, Box\u003cdyn std::error::Error\u003e\u003e\n\n// Optional helpers (if useful):\npub fn get_schema\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cString, Box\u003cdyn std::error::Error\u003e\u003e\npub fn get_row_count\u003cP: AsRef\u003cPath\u003e\u003e(path: P) -\u003e Result\u003cusize, Box\u003cdyn std::error::Error\u003e\u003e\n```\n\n### Acceptance Criteria\n- [ ] `src/parquet.rs` created with both feature-enabled and dummy implementations\n- [ ] `src/lib.rs` exports parquet module conditionally\n- [ ] `cargo build --release` succeeds (parquet module not compiled)\n- [ ] `cargo build --release --features parquet` succeeds\n- [ ] Dummy implementation returns helpful error message\n- [ ] No clippy warnings: `cargo clippy --features parquet`\n\n### Verification Commands\n```bash\ncargo build --release\ncargo build --release --features parquet\ncargo clippy --features parquet\n```\n\n### DO NOT\n- Implement the full type conversion yet (that's cpf.4)\n- Create a simple stub that compiles, with TODO comments for the conversion logic\n- Add any main.rs changes (that's cpf.3)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-22T11:11:55.170435Z","updated_at":"2026-01-24T12:46:03.601888Z","dependencies":[{"issue_id":"bytefreq-cpf.2","depends_on_id":"bytefreq-cpf","type":"parent-child","created_at":"2026-01-22T11:11:55.170781Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-cpf.2","depends_on_id":"bytefreq-cpf.1","type":"blocks","created_at":"2026-01-22T11:12:00.25444Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-cpf.3","title":"Integrate Parquet format in main.rs CLI","description":"## Task: Integrate Parquet format in main.rs CLI\n\n### Files to Modify\n- `src/main.rs`\n\n### Reference Pattern Locations\n- CLI args for format: lines 706-716\n- CLI args for excel_path: lines 718-723\n- Data loading branch: lines 846-872\n- Format routing: line 917\n\n### Exact Changes Required\n\n#### 1. Update format help text (line 710-713)\nAdd parquet to the format options:\n```rust\n.help(\"Sets the format of the input data:\\n\\\n       'json' - JSON data (each line should contain a JSON object)\\n\\\n       'tabular' - Tabular data (first line should be the header)\\n\\\n       'excel' - Excel file (.xlsx, .xls, .xlsb, .ods) - requires --excel-path\\n\\\n       'parquet' - Parquet file (.parquet) - requires --parquet-path\")\n```\n\n#### 2. Add parquet_path argument (after line 723, before sheet arg)\nFollow the excel_path pattern:\n```rust\n.arg(\n    Arg::new(\"parquet_path\")\n        .long(\"parquet-path\")\n        .value_name(\"PARQUET_PATH\")\n        .help(\"Path to Parquet file (required when format is 'parquet')\")\n        .takes_value(true),\n)\n```\n\n#### 3. Add import at top of file (near other use statements)\n```rust\nuse bytefreq::parquet::ParquetReader;\n```\n\n#### 4. Add parquet branch in data loading (after line 869, before the else)\nInsert new branch BEFORE the else clause:\n```rust\n} else if format == \"parquet\" {\n    // Parquet processing - convert to JSON lines\n    let parquet_path = matches.value_of(\"parquet_path\")\n        .expect(\"--parquet-path is required when format is 'parquet'\");\n    \n    ParquetReader::read_as_json_lines(parquet_path)\n        .expect(\"Failed to read Parquet file\")\n} else {\n```\n\n#### 5. Update format routing (line 917)\nChange the actual_format logic to route parquet to json:\n```rust\nlet actual_format = match format {\n    \"excel\" =\u003e \"tabular\",\n    \"parquet\" =\u003e \"json\",  // Parquet uses JSON processing pipeline\n    _ =\u003e format,\n};\n```\n\n### Critical: NO header processing for Parquet\nParquet has embedded schema, so it should NOT go through the tabular header processing block (lines 893-911). The check `if format == \"tabular\" || format == \"excel\"` must NOT include parquet.\n\n### Acceptance Criteria\n- [ ] `--format parquet` accepted as valid format\n- [ ] `--parquet-path` argument available\n- [ ] Running with `--format parquet` without `--parquet-path` produces helpful error\n- [ ] Parquet data routes to JSON processing (not tabular)\n- [ ] `cargo build --release --features parquet` succeeds\n- [ ] Help text shows parquet option: `./target/release/bytefreq --help`\n\n### Verification Commands\n```bash\ncargo build --release --features parquet\n./target/release/bytefreq --help | grep -A5 \"format\"\n./target/release/bytefreq --help | grep \"parquet-path\"\n# This should error helpfully (no parquet-path):\n./target/release/bytefreq -f parquet \u003c /dev/null\n```\n\n### DO NOT\n- Modify the JSON processing logic\n- Add sheet/header_row options for parquet (not applicable)\n- Change existing excel or tabular behavior","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-22T11:11:55.307548Z","updated_at":"2026-01-24T13:06:02.919853Z","dependencies":[{"issue_id":"bytefreq-cpf.3","depends_on_id":"bytefreq-cpf","type":"parent-child","created_at":"2026-01-22T11:11:55.307909Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-cpf.3","depends_on_id":"bytefreq-cpf.1","type":"blocks","created_at":"2026-01-22T11:12:00.280106Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-cpf.4","title":"Handle Parquet nested types to JSON conversion","description":"## Task: Handle Parquet nested types to JSON conversion\n\n### Files to Modify\n- `src/parquet.rs` (complete the `read_as_json_lines` implementation)\n\n### Context\nThis is the core logic that converts Parquet rows into JSON strings that feed into `process_json_line()`. The JSON processor already handles nested structures with dot-notation paths.\n\n### Reference Pattern\nFollow `src/excel.rs:84-107` cell_to_string() pattern, but output JSON values instead of strings.\n\n### Arrow Type to JSON Mapping\n\n| Arrow Type | JSON Type | Example |\n|------------|-----------|---------|\n| Int8/16/32/64, UInt8/16/32/64 | Number | `42` |\n| Float32/64 | Number | `3.14` |\n| Utf8, LargeUtf8 | String | `\"hello\"` |\n| Boolean | Boolean | `true` |\n| Null | Null | `null` |\n| Struct | Object | `{\"city\": \"NYC\", \"zip\": \"10001\"}` |\n| List, LargeList | Array | `[1, 2, 3]` |\n| Map | Object | `{\"key1\": \"value1\"}` |\n| Timestamp | String (ISO8601) | `\"2024-01-15T10:30:00Z\"` |\n| Date32/64 | String | `\"2024-01-15\"` |\n| Binary, LargeBinary | String (base64) | `\"SGVsbG8=\"` |\n\n### Implementation Approach\n\n```rust\nuse arrow::array::*;\nuse arrow::datatypes::*;\nuse serde_json::{json, Value, Map};\n\nfn record_batch_to_json_lines(batch: \u0026RecordBatch) -\u003e Vec\u003cString\u003e {\n    let schema = batch.schema();\n    let num_rows = batch.num_rows();\n    let mut lines = Vec::with_capacity(num_rows);\n    \n    for row_idx in 0..num_rows {\n        let mut row_obj = Map::new();\n        \n        for (col_idx, field) in schema.fields().iter().enumerate() {\n            let column = batch.column(col_idx);\n            let value = array_value_to_json(column.as_ref(), row_idx, field.data_type());\n            row_obj.insert(field.name().clone(), value);\n        }\n        \n        lines.push(serde_json::to_string(\u0026Value::Object(row_obj)).unwrap());\n    }\n    \n    lines\n}\n\nfn array_value_to_json(array: \u0026dyn Array, row_idx: usize, data_type: \u0026DataType) -\u003e Value {\n    if array.is_null(row_idx) {\n        return Value::Null;\n    }\n    \n    match data_type {\n        DataType::Boolean =\u003e {\n            let arr = array.as_any().downcast_ref::\u003cBooleanArray\u003e().unwrap();\n            json!(arr.value(row_idx))\n        }\n        DataType::Int32 =\u003e {\n            let arr = array.as_any().downcast_ref::\u003cInt32Array\u003e().unwrap();\n            json!(arr.value(row_idx))\n        }\n        DataType::Int64 =\u003e {\n            let arr = array.as_any().downcast_ref::\u003cInt64Array\u003e().unwrap();\n            json!(arr.value(row_idx))\n        }\n        DataType::Float64 =\u003e {\n            let arr = array.as_any().downcast_ref::\u003cFloat64Array\u003e().unwrap();\n            json!(arr.value(row_idx))\n        }\n        DataType::Utf8 =\u003e {\n            let arr = array.as_any().downcast_ref::\u003cStringArray\u003e().unwrap();\n            json!(arr.value(row_idx))\n        }\n        DataType::Struct(fields) =\u003e {\n            let struct_arr = array.as_any().downcast_ref::\u003cStructArray\u003e().unwrap();\n            let mut obj = Map::new();\n            for (i, field) in fields.iter().enumerate() {\n                let child_array = struct_arr.column(i);\n                let child_value = array_value_to_json(child_array.as_ref(), row_idx, field.data_type());\n                obj.insert(field.name().clone(), child_value);\n            }\n            Value::Object(obj)\n        }\n        DataType::List(field) =\u003e {\n            let list_arr = array.as_any().downcast_ref::\u003cListArray\u003e().unwrap();\n            let values = list_arr.value(row_idx);\n            let mut arr = Vec::new();\n            for i in 0..values.len() {\n                arr.push(array_value_to_json(values.as_ref(), i, field.data_type()));\n            }\n            Value::Array(arr)\n        }\n        // Add more types as needed...\n        _ =\u003e json!(format!(\"\u003cunsupported: {:?}\u003e\", data_type))\n    }\n}\n```\n\n### Expected JSON Output Examples\n\n**Flat Parquet:**\n```json\n{\"id\": 1, \"name\": \"Alice\", \"age\": 30}\n{\"id\": 2, \"name\": \"Bob\", \"age\": 25}\n```\n\n**Nested Parquet (struct column):**\n```json\n{\"user\": {\"name\": \"Alice\", \"address\": {\"city\": \"NYC\", \"zip\": \"10001\"}}, \"active\": true}\n```\n→ bytefreq will create paths: `user.name`, `user.address.city`, `user.address.zip`, `active`\n\n**Parquet with arrays:**\n```json\n{\"id\": 1, \"tags\": [\"rust\", \"data\"], \"scores\": [95, 87, 92]}\n```\n→ bytefreq will create paths: `id`, `tags[0]`, `tags[1]`, `scores[0]`, etc.\n\n### Priority Type Support\n\n**Must have (P0):**\n- Int32, Int64, UInt32, UInt64\n- Float32, Float64  \n- Utf8 (strings)\n- Boolean\n- Null\n- Struct (nested objects)\n- List (arrays)\n\n**Should have (P1):**\n- Timestamp (as ISO8601 string)\n- Date32, Date64\n- LargeUtf8, LargeList\n\n**Nice to have (P2):**\n- Map\n- Binary (as base64)\n- Decimal\n- Duration\n\n### Acceptance Criteria\n- [ ] Flat Parquet files produce valid JSON lines\n- [ ] Nested structs produce nested JSON objects\n- [ ] Lists produce JSON arrays\n- [ ] Null values produce JSON null\n- [ ] Timestamps formatted as ISO8601 strings\n- [ ] Unknown types produce `\u003cunsupported: Type\u003e` instead of panic\n- [ ] `cargo test --features parquet` passes\n\n### Test Cases to Add\n```rust\n#[cfg(test)]\n#[cfg(feature = \"parquet\")]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_flat_parquet() {\n        // Test with a simple flat parquet file\n    }\n    \n    #[test]\n    fn test_nested_struct() {\n        // Test with nested struct columns\n    }\n    \n    #[test]\n    fn test_list_column() {\n        // Test with list/array columns\n    }\n}\n```\n\n### Verification Commands\n```bash\ncargo build --release --features parquet\ncargo test --features parquet\ncargo clippy --features parquet\n\n# Integration test (if you have a test parquet file):\n./target/release/bytefreq -f parquet --parquet-path testdata/sample.parquet -g LU\n```\n\n### DO NOT\n- Panic on unsupported types (return placeholder string)\n- Skip null handling (must produce JSON null)\n- Flatten nested structures yourself (let process_json_line handle paths)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-22T11:11:55.447067Z","updated_at":"2026-01-24T13:26:30.755726Z","dependencies":[{"issue_id":"bytefreq-cpf.4","depends_on_id":"bytefreq-cpf","type":"parent-child","created_at":"2026-01-22T11:11:55.447427Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-cpf.4","depends_on_id":"bytefreq-cpf.2","type":"blocks","created_at":"2026-01-22T11:12:00.305389Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-cpf.5","title":"Test Parquet support with sample files","description":"## Task: Test Parquet support with sample files\n\n### Files to Create\n- `testdata/sample_flat.parquet` - flat columns only\n- `testdata/sample_nested.parquet` - with struct columns\n- `testdata/sample_arrays.parquet` - with list columns\n\n### Test Scenarios\n\n#### 1. Flat Parquet (basic functionality)\nCreate a parquet file with columns: id (int), name (string), value (float), active (bool)\n\n**Expected output pattern:**\n```\ncol_00000_id     |  9      | 100 | 5\ncol_00001_name   |  Aaaa   | 50  | Alice\ncol_00001_name   |  Aaa    | 50  | Bob\ncol_00002_value  |  9.99   | 100 | 3.14\ncol_00003_active |  true   | 60  | true\ncol_00003_active |  false  | 40  | false\n```\n\n#### 2. Nested Struct (JSON path generation)\nCreate parquet with struct column: user { name: string, address: { city: string, zip: string } }\n\n**Expected paths in output:**\n- `user.name`\n- `user.address.city`\n- `user.address.zip`\n\n#### 3. Array/List columns\nCreate parquet with list column: tags (list\u003cstring\u003e), scores (list\u003cint\u003e)\n\n**Expected paths with array indices:**\n- `tags[0]`, `tags[1]`, etc.\n- `scores[0]`, `scores[1]`, etc.\n\nOr with `-a` flag (remove array numbers):\n- `tags[]`\n- `scores[]`\n\n#### 4. WASM compilation check\nVerify the feature compiles to wasm32 target.\n\n### Creating Test Parquet Files\n\nOption A: Use Python with pyarrow\n```python\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# Flat table\nflat_table = pa.table({\n    'id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'value': [1.5, 2.5, 3.5],\n    'active': [True, False, True]\n})\npq.write_table(flat_table, 'testdata/sample_flat.parquet')\n\n# Nested struct\nnested_table = pa.table({\n    'user': [\n        {'name': 'Alice', 'address': {'city': 'NYC', 'zip': '10001'}},\n        {'name': 'Bob', 'address': {'city': 'LA', 'zip': '90001'}},\n    ]\n})\npq.write_table(nested_table, 'testdata/sample_nested.parquet')\n\n# Array columns\narray_table = pa.table({\n    'id': [1, 2],\n    'tags': [['rust', 'data'], ['python', 'ml']],\n    'scores': [[95, 87], [88, 92, 79]]\n})\npq.write_table(array_table, 'testdata/sample_arrays.parquet')\n```\n\nOption B: Download existing test files\n```bash\n# Apache Arrow test data\ncurl -o testdata/sample.parquet https://github.com/apache/parquet-testing/raw/master/data/alltypes_plain.parquet\n```\n\n### Verification Commands\n\n```bash\n# Build with parquet feature\ncargo build --release --features parquet\n\n# Test flat parquet - DQ report\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_flat.parquet\n\n# Test with different grains\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_flat.parquet -g L\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_flat.parquet -g LU\n\n# Test nested - verify dot paths appear\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_nested.parquet\n\n# Test arrays - verify array indexing\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_arrays.parquet\n\n# Test arrays with -a flag (remove array numbers)\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_arrays.parquet -a\n\n# Test enhanced output\n./target/release/bytefreq -f parquet --parquet-path testdata/sample_flat.parquet -e\n\n# WASM compilation\nrustup target add wasm32-unknown-unknown\ncargo build --release --features parquet --target wasm32-unknown-unknown\n\n# Run unit tests\ncargo test --features parquet\n```\n\n### Acceptance Criteria\n- [ ] Flat parquet produces expected DQ output\n- [ ] Nested struct columns produce dot-notation paths (e.g., `user.address.city`)\n- [ ] List columns produce indexed paths (e.g., `tags[0]`)\n- [ ] `-a` flag removes array numbers correctly\n- [ ] Enhanced output (`-e`) works with parquet\n- [ ] Error message shown when parquet feature not enabled\n- [ ] WASM compilation succeeds: `cargo build --features parquet --target wasm32-unknown-unknown`\n- [ ] All unit tests pass: `cargo test --features parquet`\n\n### Regression Checks\nEnsure existing functionality still works:\n```bash\n# Tabular still works\ncat testdata/test.pip | ./target/release/bytefreq\n\n# JSON still works  \ncat testdata/example.json | ./target/release/bytefreq -f json\n\n# Excel still works (if excel feature enabled)\ncargo build --release --features excel\n./target/release/bytefreq -f excel --excel-path testdata/Illegal_Dumping_Incidents.xls --sheet 0\n```\n\n### DO NOT\n- Skip WASM compilation test\n- Test only happy path (test error cases too)\n- Modify existing test data files","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-22T11:11:55.585406Z","updated_at":"2026-01-24T13:26:58.569874Z","dependencies":[{"issue_id":"bytefreq-cpf.5","depends_on_id":"bytefreq-cpf","type":"parent-child","created_at":"2026-01-22T11:11:55.585799Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-cpf.5","depends_on_id":"bytefreq-cpf.3","type":"blocks","created_at":"2026-01-22T11:12:00.330861Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-cpf.5","depends_on_id":"bytefreq-cpf.4","type":"blocks","created_at":"2026-01-22T11:12:00.356159Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-dq0","title":"Add 'excel' format option to CLI","description":"Update main.rs CLI argument parsing:\n- Add 'excel' as a valid value for --format/-f flag\n- Update help text to document Excel support\n- Add --sheet/-s option to specify which sheet to process (default: first sheet)\n- Add --sheet-name option to process sheet by name instead of index\n- Ensure format validation includes 'excel'\n\nCLI should support:\nbytefreq -f excel \u003c file.xlsx\nbytefreq -f excel --sheet 2 \u003c file.xlsx  \nbytefreq -f excel --sheet-name \"Sales\" \u003c file.xlsx","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:12.04681Z","updated_at":"2025-12-12T19:41:15.333446Z","closed_at":"2025-12-12T19:41:15.333446Z","dependencies":[{"issue_id":"bytefreq-dq0","depends_on_id":"bytefreq-55j","type":"blocks","created_at":"2025-12-12T19:20:24.161905Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-ilt","title":"Integrate full bytefreq profiling into WASM","description":"Replace minimal profile_csv() with actual bytefreq profiling logic.\n\nNeed to adapt:\n- Masking functions (HU, LU) - already Unicode-safe\n- Assertion rules engine\n- CSV parser integration\n- JSON output formatting\n\nChallenges:\n- Remove filesystem dependencies (glob, walkdir)\n- Handle stdin alternative (use string buffer)\n- Ensure rayon works in WASM or replace with single-threaded\n- Strip clap CLI parsing","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-13T01:07:49.479764Z","updated_at":"2025-12-13T01:07:49.479764Z","dependencies":[{"issue_id":"bytefreq-ilt","depends_on_id":"bytefreq-nvd","type":"blocks","created_at":"2025-12-13T01:07:55.933705Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-l05","title":"Integrate Excel processing into main event loop","description":"Modify main.rs to:\n- Detect 'excel' format and route to Excel processing path\n- Read Excel file from stdin or file path\n- Feed Excel rows into existing tabular processing pipeline\n- Reuse existing frequency_maps, example_maps, column_names logic\n- Support enhanced output modes (-e, -E) for Excel data\n- Handle errors gracefully (invalid Excel files, missing sheets, etc.)\n\nIntegration points:\n- Use existing Arc\u003cMutex\u003c\u003e\u003e shared state\n- Leverage Rayon for parallel row processing if beneficial\n- Maintain compatibility with existing DQ and CP report modes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:12.237993Z","updated_at":"2025-12-12T22:36:05.660243Z","closed_at":"2025-12-12T22:36:05.660243Z","dependencies":[{"issue_id":"bytefreq-l05","depends_on_id":"bytefreq-55j","type":"blocks","created_at":"2025-12-12T19:20:24.300427Z","created_by":"daemon","metadata":"{}"},{"issue_id":"bytefreq-l05","depends_on_id":"bytefreq-dq0","type":"blocks","created_at":"2025-12-12T19:20:24.330576Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-nvd","title":"Build bytefreq as WASM module","description":"Compile bytefreq to WebAssembly for browser execution.\n\nSteps:\n- Add wasm-bindgen and console_error_panic_hook dependencies\n- Create lib.rs with WASM-compatible profile_csv function\n- Configure Cargo.toml with cdylib crate type\n- Build for wasm32-unknown-unknown target\n- Generate JavaScript bindings with wasm-bindgen\n\nDependencies already added, build in progress.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T01:07:31.743803Z","updated_at":"2025-12-13T01:28:22.289276Z","closed_at":"2025-12-13T01:28:22.289276Z"}
{"id":"bytefreq-p9s","title":"Add tests for Excel functionality","description":"Create tests for Excel processing:\n- Unit tests for src/excel.rs module\n- Test reading different Excel formats (.xlsx, .xls)\n- Test sheet selection (by index and name)\n- Test data type conversion\n- Test error handling (invalid files, missing sheets)\n- Integration test with sample Excel file in testdata/\n- Test enhanced output modes with Excel input\n\nAdd sample Excel test files to testdata/ directory.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T19:20:12.806848Z","updated_at":"2025-12-12T21:42:52.550705Z","closed_at":"2025-12-12T21:42:52.550705Z","dependencies":[{"issue_id":"bytefreq-p9s","depends_on_id":"bytefreq-l05","type":"blocks","created_at":"2025-12-12T19:20:24.666674Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-r6q","title":"Create minimal HTML interface for WASM profiler","description":"Build simple web page that loads WASM module and profiles local files.\n\nRequirements:\n- File input (CSV/Excel)\n- Load WASM module\n- Call profile_csv() with file contents\n- Display report in \u003cpre\u003e tag\n- No server upload - all processing client-side\n\nPrivacy benefit: Data never leaves user's laptop.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T01:07:39.384965Z","updated_at":"2025-12-13T01:29:31.510066Z","closed_at":"2025-12-13T01:29:31.510066Z","dependencies":[{"issue_id":"bytefreq-r6q","depends_on_id":"bytefreq-nvd","type":"blocks","created_at":"2025-12-13T01:07:55.904633Z","created_by":"daemon","metadata":"{}"}]}
{"id":"bytefreq-yif","title":"Replace naive delimiter splitting with csv crate parser","description":"Replace string.split(delimiter) with proper CSV parsing using the csv crate (already in Cargo.toml).\n\nCurrent problem:\n- Naive .split(delimiter) doesn't handle quoted fields with embedded commas\n- Causes ragged data issues on Excel-exported CSV files\n- Fields like \"Smith, John\" are incorrectly split into two fields\n\nSolution:\n- Use csv::ReaderBuilder with configurable delimiter\n- Handle quoted fields, escaped quotes, and proper field boundaries\n- Apply to both header parsing and data row parsing\n- Maintain compatibility with existing delimiter parameter","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T22:06:49.476059Z","updated_at":"2025-12-12T22:36:05.65909Z","closed_at":"2025-12-12T22:36:05.65909Z"}
